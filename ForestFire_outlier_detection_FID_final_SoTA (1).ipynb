{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE5ZfugDBSzG"
      },
      "outputs": [],
      "source": [
        "# # #  Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, LSTM, Attention, Add, Reshape, RepeatVector, Concatenate, Permute\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Layer, MultiHeadAttention, Add, LayerNormalization, Dropout\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EOV9ulze-r4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36acbc75-58b6-49af-a0f7-4bdcf4727a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # #  import data\n",
        "df = pd.read_csv('/content/drive/MyDrive/labtest/forestfires.csv')"
      ],
      "metadata": {
        "id": "yvSwLPs4CJO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #  Parameter configurations\n",
        "SIGMA = 0.5\n",
        "PERCENTAGE_OUTLIER = 6*8\n",
        "OUTLIER_PECENTAGE = 5\n",
        "PERCENTAGE_REPAIRED = 5*8\n",
        "\n",
        "length = df.shape[0]\n",
        "\n",
        "columns = df.columns.tolist()\n"
      ],
      "metadata": {
        "id": "C-pYWygRCN0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Preprocessing data\n",
        "def day_name_to_num(day_name):\n",
        "    day_list = {'mon':1, 'tue':2, 'wed':3, 'thu':4, 'fri':5, 'sat':6, 'sun':7}\n",
        "    for item, num in day_list.items():\n",
        "        if day_name == item:\n",
        "            return(num)\n",
        "\n",
        "df['month'] = pd.to_datetime(df.month, format='%b').dt.month\n",
        "\n",
        "# Convert day to numerial\n",
        "for i in range(length):\n",
        "    df.loc[i,'day'] = day_name_to_num(df.loc[i, 'day'])\n",
        "\n",
        "# Label the outlier and inlier\n",
        "for i in range(length):\n",
        "    df.loc[i, 'index'] = i\n",
        "    df.loc[i, 'label'] = 1\n",
        "\n",
        "# Select random index for outlier\n",
        "random_indices = np.random.choice(df.index, PERCENTAGE_OUTLIER, replace = False)\n",
        "\n",
        "# Creating outlier data (temperature or humidity)\n",
        "for index in random_indices:\n",
        "    df_1 = df.iloc[index]\n",
        "    temp = df_1['RH']\n",
        "\n",
        "    sigma_T = SIGMA * temp\n",
        "    noise_T = random.gauss(0, sigma_T)\n",
        "\n",
        "    temp += noise_T\n",
        "    df_1['RH']  = temp\n",
        "    df_1['label'] = 0\n",
        "\n",
        "    length +=1\n",
        "    df.loc[length] = df_1\n",
        "    # df.loc[index] = df_1\n",
        "\n",
        "# Shuffling the dataset\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8KtywCMfB868"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # FID for repairing data\n",
        "# def FID_repaired(working_list):\n",
        "#     # working_list = x1\n",
        "\n",
        "#     index = []\n",
        "#     for i, j in enumerate(working_list):\n",
        "#         if j == 'NaN':\n",
        "#             index.append(i)\n",
        "\n",
        "#     # count the number of NaN/compromised point\n",
        "#     p1 = working_list.count('NaN')\n",
        "#     # print(p1)\n",
        "\n",
        "#     t = p1 # Total number of missing value\n",
        "\n",
        "#     # Select the min & max from list\n",
        "#     # working_list.remove('NaN')\n",
        "#     count=0\n",
        "#     for index_pos in index:\n",
        "#         working_list.pop(index_pos-count)\n",
        "#         count+=1\n",
        "\n",
        "\n",
        "#     # find mean of all observed values\n",
        "#     mean = np.mean(working_list)\n",
        "\n",
        "\n",
        "#     #find min value\n",
        "#     a = min(working_list)\n",
        "\n",
        "#     #find max value\n",
        "#     b = max(working_list)\n",
        "\n",
        "#     # Calculate h = (b-a)/t\n",
        "#     h = (b-a)/t\n",
        "\n",
        "#     # Calculate the discrete universe U using u = (a + (s-1) x h + a + s x h)/2, s=1,2,3\n",
        "#     U = []\n",
        "#     for s in range(1,t+1):\n",
        "#         u = (a + (s-1) * h + a + s * h)/2\n",
        "#         U.append(u)\n",
        "\n",
        "#     # print(U)\n",
        "\n",
        "#     # Calculating the missing values\n",
        "#     M = []\n",
        "#     for u in U:\n",
        "#         # print(U)\n",
        "\n",
        "#         # Compute the contribution weight (micro) of each observed element x_i\n",
        "\n",
        "#         contribution_weight_list = []\n",
        "\n",
        "#         for i in working_list:\n",
        "#             if abs(i-u) <= h:\n",
        "#                 temp = 1-(abs(i-u)/h)\n",
        "#             else:\n",
        "#                 temp = 0\n",
        "#             contribution_weight_list.append(temp)\n",
        "\n",
        "#         # Calculate the sum of x_i to u1:\n",
        "#         sum_contribution_weight_list = sum(contribution_weight_list)\n",
        "#         # print(sum_contribution_weight_list)\n",
        "\n",
        "#         # Calculate the contribution of an observed data x_i\n",
        "#         sum_contribution_observed_data = []\n",
        "\n",
        "#         for num1, num2 in zip(working_list, contribution_weight_list):\n",
        "#         \tsum_contribution_observed_data.append(num1 * num2)\n",
        "\n",
        "#         sum_contribution_observed_data = sum(sum_contribution_observed_data)\n",
        "#         # print(sum_contribution_observed_data)\n",
        "\n",
        "#         # Calculate the missing values in x_i\n",
        "#         if sum_contribution_weight_list == 0:\n",
        "#             m = mean\n",
        "#         else:\n",
        "#             m = sum_contribution_observed_data/sum_contribution_weight_list\n",
        "\n",
        "#         M.append(m)\n",
        "\n",
        "#     # print('The values:',M)\n",
        "#     # print('The index position:',index)\n",
        "#     return [index,M]"
      ],
      "metadata": {
        "id": "j3tVt72psZFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # Modified the dataset\n",
        "# data = df['RH'].to_list()\n",
        "\n",
        "# # # # Determine the outlier for repairing\n",
        "# index_outlier = []\n",
        "\n",
        "# y = df['label']\n",
        "# count_outlier = 0\n",
        "# for i, j in enumerate(y):\n",
        "#     if j == 0.0:\n",
        "#         index_outlier.append(i)\n",
        "#         count_outlier+=1\n",
        "#     if count_outlier == PERCENTAGE_REPAIRED: break\n",
        "\n",
        "# # Determine the compromised data\n",
        "# for ind in index_outlier:\n",
        "#     data[ind] = 'NaN'\n",
        "\n",
        "# # Recover compromised data\n",
        "# working_list = data\n",
        "\n",
        "# results = FID_repaired(working_list)\n",
        "\n",
        "# # print('The recovered values:',results[1])\n",
        "# # print('The index position:',results[0])\n"
      ],
      "metadata": {
        "id": "IBwPX9ejDOGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # Update the predicted data into dataset\n",
        "# pos = 0\n",
        "# for index_pos in results[0]:\n",
        "#   df_1 = df.iloc[index_pos]\n",
        "#   df_1['RH'] = results[1][pos]\n",
        "#   df_1['label'] = 1\n",
        "\n",
        "#   df.loc[index_pos] = df_1\n",
        "#   pos+=1"
      ],
      "metadata": {
        "id": "wywVPakZtkdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper 1:\n",
        "\n",
        "# Implementation of AMSA-VAE method for missing data filling\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(df[['RH']])\n",
        "\n",
        "# Imputation\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "data_imputed = imputer.fit_transform(data_scaled)\n",
        "\n",
        "# AMSA-VAE Model Definition\n",
        "input_layer = Input(shape=(1,))\n",
        "dense_1 = Dense(128, activation='relu')(input_layer)\n",
        "repeat_vector = RepeatVector(1)(dense_1)\n",
        "permute_layer = Permute((2, 1))(repeat_vector)\n",
        "attention_output = Attention()([permute_layer, permute_layer])\n",
        "flatten_attention = Reshape((128,))(attention_output)\n",
        "add_layer = Add()([dense_1, flatten_attention])\n",
        "output_layer = Dense(1)(add_layer)\n",
        "\n",
        "# Compile and Train the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(data_imputed, data_imputed, epochs=10, batch_size=16)\n",
        "\n",
        "# Predict and Fill Missing Data\n",
        "data_filled = model.predict(data_imputed)\n",
        "data_filled = scaler.inverse_transform(data_filled)\n",
        "df['RH'] = data_filled.flatten()  # Update the DataFrame with repaired data\n"
      ],
      "metadata": {
        "id": "2d4AQdyX3jJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce624a02-d326-443e-8885-4d57001ff6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1409\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3792\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0580\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0224\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0191\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0164\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0151\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0086\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0080\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 2:\n",
        "\n",
        "# # # # Applying Switching Triple-Weight-SMOTE (NSS)\n",
        "# # Step 1: Impute missing values using LNMF\n",
        "# nmf_model = NMF(n_components=5, init='random', random_state=0)\n",
        "# W = nmf_model.fit_transform(np.nan_to_num(df[['RH']]))\n",
        "# H = nmf_model.components_\n",
        "# df['RH'] = np.dot(W, H).flatten()  # Impute missing values and update the DataFrame\n",
        "\n",
        "# # Step 2: Map to Empirical Feature Space (EFS) - simplified for demonstration\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['RH']])\n",
        "# kmeans = KMeans(n_clusters=2, random_state=0).fit(data_scaled)\n",
        "# df['cluster'] = kmeans.labels_\n",
        "\n",
        "# # Step 3: Apply Switching Triple-Weight-SMOTE\n",
        "# # Assign synthetic samples according to cluster properties\n",
        "# for cluster_label in df['cluster'].unique():\n",
        "#     cluster_data = df[df['cluster'] == cluster_label]\n",
        "#     cluster_center = cluster_data[['RH']].mean().values\n",
        "#     distances, indices = pairwise_distances_argmin_min(cluster_data[['RH']], [cluster_center])\n",
        "#     # Synthesize new samples if needed based on cluster distribution\n",
        "#     if cluster_label == 0:  # Example condition\n",
        "#         new_samples = cluster_data.sample(frac=0.1, replace=True)\n",
        "#         df = pd.concat([df, new_samples])\n",
        "\n",
        "# # Shuffle the final DataFrame\n",
        "# df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hLZU4SBx6RmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 3:\n",
        "\n",
        "# # AMSA-VAE implementation for data filling\n",
        "# class AMSA(Layer):\n",
        "#     def __init__(self, d_model, num_heads):\n",
        "#         super(AMSA, self).__init__()\n",
        "#         self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "#         self.norm = LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout = Dropout(0.1)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         # Expand dims to create the necessary 3D input (batch, seq_len, features)\n",
        "#         x = tf.expand_dims(x, axis=1)  # Expands to (batch, 1, features)\n",
        "#         attn_output = self.mha(x, x)  # Apply multi-head attention\n",
        "#         attn_output = self.dropout(attn_output)\n",
        "#         out1 = self.norm(x + attn_output)  # Add & normalize\n",
        "#         out1 = tf.squeeze(out1, axis=1)  # Squeeze back to 2D if necessary\n",
        "#         return out1\n",
        "\n",
        "# def create_amsa_vae(input_shape):\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     x = AMSA(d_model=128, num_heads=4)(inputs)\n",
        "#     x = Dense(64, activation='relu')(x)\n",
        "#     outputs = Dense(input_shape[0], activation='linear')(x)\n",
        "#     model = Model(inputs, outputs)\n",
        "#     return model\n",
        "\n",
        "# # Data scaling\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['RH']])\n",
        "\n",
        "# # AMSA-VAE model setup and training\n",
        "# amsa_vae = create_amsa_vae((1,))\n",
        "# amsa_vae.compile(optimizer='adam', loss='mse')\n",
        "# amsa_vae.fit(data_scaled, data_scaled, epochs=10, batch_size=4)\n",
        "\n",
        "# # Predict and update filled data\n",
        "# data_filled = amsa_vae.predict(data_scaled)\n",
        "# df['RH'] = scaler.inverse_transform(data_filled).flatten()  # Update the DataFrame with repaired data"
      ],
      "metadata": {
        "id": "lUOmZzdQ6koS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Splitting the dataset\n",
        "y = df['label']\n",
        "X = df.drop(['index', 'label'], axis =1)\n",
        "\n",
        "# print(np.count_nonzero(y == 0))\n",
        "\n",
        "# Spliting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y)\n"
      ],
      "metadata": {
        "id": "_nJDd_aDxKM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # Random Forest\n",
        "# forest = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
        "# forest.fit(x_train, y_train)\n",
        "# preds = forest.predict(x_test)\n",
        "\n",
        "\n",
        "# # # Metrics\n",
        "# print('accuracy_score: ', round((accuracy_score(y_test,preds)*100),2))\n",
        "# print('f1_score: ', round(f1_score(y_test,preds),6))\n",
        "# print('precision_score: ', round(precision_score(y_test,preds),6))\n",
        "# print('recall_score: ', round(recall_score(y_test,preds),6))\n",
        "# tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
        "# specificity = tn/(tn+fp)\n",
        "# print('Specificity : ', round(specificity,6))"
      ],
      "metadata": {
        "id": "gC5qz7ugDZAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # Autoencoder\n",
        "\n",
        "# x_train = np.asarray(x_train).astype('float32')\n",
        "# x_test = np.asarray(x_test).astype('float32')\n",
        "# y_train = np.asarray(y_train).astype('float32')\n",
        "# y_test = np.asarray(y_test).astype('float32')\n",
        "\n",
        "# # Splitting the for testing and validating dataset\n",
        "# x_val, x_test, y_val, y_test = train_test_split(x_test,y_test, test_size=0.33,stratify=y_test)\n",
        "\n",
        "# class AutoEncoder(Model):\n",
        "#   \"\"\"\n",
        "#   Parameters\n",
        "#   ----------\n",
        "#   output_units: int\n",
        "#     Number of output units\n",
        "\n",
        "#   code_size: int\n",
        "#     Number of units in bottle neck\n",
        "#   \"\"\"\n",
        "\n",
        "#   def __init__(self, output_units, code_size=8):\n",
        "#     super().__init__()\n",
        "#     self.encoder = Sequential([\n",
        "#       Dense(64, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(32, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(16, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(code_size, activation='relu')\n",
        "#     ])\n",
        "#     self.decoder = Sequential([\n",
        "#       Dense(16, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(32, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(64, activation='relu'),\n",
        "#       Dropout(0.1),\n",
        "#       Dense(output_units, activation='sigmoid')\n",
        "#     ])\n",
        "\n",
        "#   def call(self, inputs):\n",
        "#     encoded = self.encoder(inputs)\n",
        "#     decoded = self.decoder(encoded)\n",
        "#     return decoded\n",
        "\n",
        "# model = AutoEncoder(output_units=x_train.shape[1])\n",
        "# # configurations of model\n",
        "# model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "# # simple early stopping\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "# history = model.fit(\n",
        "#     x_train,\n",
        "#     x_train,\n",
        "#     epochs=50,\n",
        "#     batch_size=128,\n",
        "#     validation_data=(x_val, x_val),\n",
        "#     verbose=0,\n",
        "#     callbacks=[es]\n",
        "# )\n",
        "\n",
        "# def find_threshold(model, x_train_scaled):\n",
        "#   # another method to find threshold\n",
        "#   reconstructions = model.predict(x_train_scaled)\n",
        "#   # provides losses of individual instances\n",
        "#   reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
        "\n",
        "#   threshold_2 = np.percentile(reconstruction_errors, 100-OUTLIER_PECENTAGE)\n",
        "#   return threshold_2\n",
        "\n",
        "# def get_predictions(model, x_test_scaled, threshold):\n",
        "#   predictions = model.predict(x_test_scaled)\n",
        "#   # provides losses of individual instances\n",
        "#   errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
        "#   # 0 = anomaly, 1 = normal\n",
        "#   anomaly_mask = pd.Series(errors) > threshold\n",
        "#   preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
        "#   return preds\n",
        "\n",
        "# threshold = find_threshold(model, x_train)\n",
        "# preds = get_predictions(model, x_test, threshold)\n",
        "\n",
        "# preds = np.asarray(preds).astype('float32')\n",
        "\n",
        "# #Metrics\n",
        "# print('accuracy_score: ', round((accuracy_score(y_test,preds)*100),2))\n",
        "# print('f1_score: ', round(f1_score(y_test,preds),6))\n",
        "# print('precision_score: ', round(precision_score(y_test,preds),6))\n",
        "# print('recall_score: ', round(recall_score(y_test,preds),6))\n",
        "# tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
        "# specificity = tn/(tn+fp)\n",
        "# print('Specificity : ', round(specificity,6))"
      ],
      "metadata": {
        "id": "Xtm95dWZRDt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # KNeighborsClassifier\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "neigh.fit(x_train, y_train)\n",
        "\n",
        "preds = neigh.predict(x_test)\n",
        "\n",
        "# Metrics\n",
        "print('accuracy_score: ', round((accuracy_score(y_test,preds)*100),2))\n",
        "print('f1_score: ', round(f1_score(y_test,preds),6))\n",
        "print('precision_score: ', round(precision_score(y_test,preds),6))\n",
        "print('recall_score: ', round(recall_score(y_test,preds),6))\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
        "specificity = tn/(tn+fp)\n",
        "print('Specificity : ', round(specificity,6))"
      ],
      "metadata": {
        "id": "zpQ5XskBH3T1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc67a059-37e1-4bd2-b267-ef1315b2614b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy_score:  91.76\n",
            "f1_score:  0.957055\n",
            "precision_score:  0.917647\n",
            "recall_score:  1.0\n",
            "Specificity :  0.0\n"
          ]
        }
      ]
    }
  ]
}