{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnLMjREJB0lu"
      },
      "outputs": [],
      "source": [
        "# # # Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLcNu1_B5xT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "74ea11a3-1144-4d08-c469-7d485ff1323a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d26e02ed546b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # #  import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/labtest/UNSW_noisy_data/UNSW_noisy_dur_02_1_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# # #  import data\n",
        "df = pd.read_csv('/content/drive/MyDrive/labtest/UNSW_noisy_data/UNSW_noisy_dur_05_6_5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsr-Z3C8CF1C"
      },
      "outputs": [],
      "source": [
        "# # #  Parameter configurations\n",
        "# SIGMA = 0.5\n",
        "# OUTLIER_PECENTAGE = 6\n",
        "PERCENTAGE_REPAIRED = 1724*5\n",
        "\n",
        "length = df.shape[0]\n",
        "\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "# Shuffle data\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ansqvyloDjVJ"
      },
      "outputs": [],
      "source": [
        "# # # # FID for repairing data\n",
        "# def FID_repaired(working_list):\n",
        "#     # working_list = x1\n",
        "\n",
        "#     index = []\n",
        "#     for i, j in enumerate(working_list):\n",
        "#         if j == 'NaN':\n",
        "#             index.append(i)\n",
        "\n",
        "#     # count the number of NaN/compromised point\n",
        "#     p1 = working_list.count('NaN')\n",
        "#     # print(p1)\n",
        "\n",
        "#     t = p1 # Total number of missing value\n",
        "\n",
        "#     # Select the min & max from list\n",
        "#     # working_list.remove('NaN')\n",
        "#     count=0\n",
        "#     for index_pos in index:\n",
        "#         working_list.pop(index_pos-count)\n",
        "#         count+=1\n",
        "\n",
        "#     # find mean of all observed values\n",
        "#     mean = np.mean(working_list)\n",
        "\n",
        "#     #find min value\n",
        "#     a = min(working_list)\n",
        "\n",
        "#     #find max value\n",
        "#     b = max(working_list)\n",
        "\n",
        "#     # Calculate h = (b-a)/t\n",
        "#     h = (b-a)/t\n",
        "\n",
        "#     # Calculate the discrete universe U using u = (a + (s-1) x h + a + s x h)/2, s=1,2,3\n",
        "#     U = []\n",
        "#     for s in range(1,t+1):\n",
        "#         u = (a + (s-1) * h + a + s * h)/2\n",
        "#         U.append(u)\n",
        "\n",
        "#     # print(U)\n",
        "\n",
        "#     # Calculating the missing values\n",
        "#     M = []\n",
        "#     for u in U:\n",
        "#         # print(U)\n",
        "\n",
        "#         # Compute the contribution weight (micro) of each observed element x_i\n",
        "\n",
        "#         contribution_weight_list = []\n",
        "\n",
        "#         for i in working_list:\n",
        "#             if abs(i-u) <= h:\n",
        "#                 temp = 1-(abs(i-u)/h)\n",
        "#             else:\n",
        "#                 temp = 0\n",
        "#             contribution_weight_list.append(temp)\n",
        "\n",
        "#         # Calculate the sum of x_i to u1:\n",
        "#         sum_contribution_weight_list = sum(contribution_weight_list)\n",
        "#         # print(sum_contribution_weight_list)\n",
        "\n",
        "#         # Calculate the contribution of an observed data x_i\n",
        "#         sum_contribution_observed_data = []\n",
        "\n",
        "#         for num1, num2 in zip(working_list, contribution_weight_list):\n",
        "#         \tsum_contribution_observed_data.append(num1 * num2)\n",
        "\n",
        "#         sum_contribution_observed_data = sum(sum_contribution_observed_data)\n",
        "#         # print(sum_contribution_observed_data)\n",
        "\n",
        "#         # Calculate the missing values in x_i\n",
        "#         if sum_contribution_weight_list == 0:\n",
        "#             m = mean\n",
        "#         else:\n",
        "#             m = sum_contribution_observed_data/sum_contribution_weight_list\n",
        "\n",
        "#         M.append(m)\n",
        "\n",
        "#     # print('The values:',M)\n",
        "#     # print('The index position:',index)\n",
        "#     return [index,M]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSCrjBILDxE1"
      },
      "outputs": [],
      "source": [
        "# # # # Repairing data\n",
        "# data = df['dur'].to_list()\n",
        "\n",
        "# # # # Determine the outlier for repairing\n",
        "# index_outlier = []\n",
        "\n",
        "# y = df['label2']\n",
        "# count_outlier = 0\n",
        "# for i, j in enumerate(y):\n",
        "#     if j == 0.0:\n",
        "#         index_outlier.append(i)\n",
        "#         count_outlier+=1\n",
        "#     if count_outlier == PERCENTAGE_REPAIRED: break\n",
        "\n",
        "# # Determine the compromised data\n",
        "# for ind in index_outlier:\n",
        "#     data[ind] = 'NaN'\n",
        "\n",
        "# # Recover compromised data\n",
        "# working_list = data\n",
        "\n",
        "# results = FID_repaired(working_list)\n",
        "\n",
        "# # print('The recovered values:',results[1])\n",
        "# # print('The index position:',results[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 1:\n",
        "\n",
        "# # Standard Scaling\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['400017']])\n",
        "\n",
        "# # Imputation using KNNImputer for initial fill\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# data_imputed = imputer.fit_transform(data_scaled)\n",
        "\n",
        "# # Initialize parameters using KMeans\n",
        "# def initialize_params(X, K):\n",
        "#     kmeans = KMeans(n_clusters=K).fit(X)\n",
        "#     mu = kmeans.cluster_centers_\n",
        "#     pi = np.array([np.mean(kmeans.labels_ == k) for k in range(K)])\n",
        "#     cov = np.array([np.cov(X[kmeans.labels_ == k].T) for k in range(K)])\n",
        "#     return pi, mu, cov\n",
        "\n",
        "# # E-step of the EM algorithm\n",
        "# def e_step(X, pi, mu, cov):\n",
        "#     N, D = X.shape\n",
        "#     K = len(pi)\n",
        "#     responsibilities = np.zeros((N, K))\n",
        "\n",
        "#     for k in range(K):\n",
        "#         pdf = multivariate_t.pdf(X, df=D, loc=mu[k], shape=cov[k])\n",
        "#         responsibilities[:, k] = pi[k] * pdf\n",
        "\n",
        "#     responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
        "#     return responsibilities\n",
        "\n",
        "# # M-step of the EM algorithm\n",
        "# def m_step(X, responsibilities):\n",
        "#     N, D = X.shape\n",
        "#     K = responsibilities.shape[1]\n",
        "#     N_k = responsibilities.sum(axis=0)\n",
        "\n",
        "#     pi = N_k / N\n",
        "#     mu = np.dot(responsibilities.T, X) / N_k[:, np.newaxis]\n",
        "#     cov = np.array([np.cov(X.T, aweights=(responsibilities[:, k])) for k in range(K)])\n",
        "\n",
        "#     return pi, mu, cov\n",
        "\n",
        "# # Main FEM algorithm for data imputation\n",
        "# def flexible_em(X, K, max_iter=100, tol=1e-6):\n",
        "#     pi, mu, cov = initialize_params(X, K)\n",
        "\n",
        "#     for i in range(max_iter):\n",
        "#         responsibilities = e_step(X, pi, mu, cov)\n",
        "#         pi_new, mu_new, cov_new = m_step(X, responsibilities)\n",
        "\n",
        "#         # Check convergence\n",
        "#         if np.linalg.norm(mu_new - mu) < tol:\n",
        "#             break\n",
        "\n",
        "#         pi, mu, cov = pi_new, mu_new, cov_new\n",
        "\n",
        "#     return pi, mu, cov, responsibilities\n",
        "\n",
        "# # Applying the EM Algorithm\n",
        "# K = 3  # Number of clusters (adjust based on the data characteristics)\n",
        "# pi, mu, cov, responsibilities = flexible_em(data_imputed, K)\n",
        "\n",
        "# # Updating the DataFrame with imputed values\n",
        "# df['400017'] = scaler.inverse_transform(data_imputed).flatten()"
      ],
      "metadata": {
        "id": "O_JvmRF6D7hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 2:\n",
        "\n",
        "# # # # Applying Switching Triple-Weight-SMOTE (NSS)\n",
        "# # Step 1: Impute missing values using LNMF\n",
        "# nmf_model = NMF(n_components=5, init='random', random_state=0)\n",
        "# W = nmf_model.fit_transform(np.nan_to_num(df[['dur']]))\n",
        "# H = nmf_model.components_\n",
        "# df['RH'] = np.dot(W, H).flatten()  # Impute missing values and update the DataFrame\n",
        "\n",
        "# # Step 2: Map to Empirical Feature Space (EFS) - simplified for demonstration\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['dur']])\n",
        "# kmeans = KMeans(n_clusters=2, random_state=0).fit(data_scaled)\n",
        "# df['cluster'] = kmeans.labels_\n",
        "\n",
        "# # Step 3: Apply Switching Triple-Weight-SMOTE\n",
        "# # Assign synthetic samples according to cluster properties\n",
        "# for cluster_label in df['cluster'].unique():\n",
        "#     cluster_data = df[df['cluster'] == cluster_label]\n",
        "#     cluster_center = cluster_data[['dur']].mean().values\n",
        "#     distances, indices = pairwise_distances_argmin_min(cluster_data[['dur']], [cluster_center])\n",
        "#     # Synthesize new samples if needed based on cluster distribution\n",
        "#     if cluster_label == 0:  # Example condition\n",
        "#         new_samples = cluster_data.sample(frac=0.1, replace=True)\n",
        "#         df = pd.concat([df, new_samples])\n",
        "\n",
        "# # Shuffle the final DataFrame\n",
        "# df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "tJGQpPI5D-dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Paper 3:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define the AMSA layer, modified to align with your structure\n",
        "class AMSA(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(AMSA, self).__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.norm = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = Dropout(0.1)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.expand_dims(x, axis=1)\n",
        "        attn_output = self.mha(x, x)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        out1 = self.norm(x + attn_output)\n",
        "        out1 = tf.squeeze(out1, axis=1)\n",
        "        return out1\n",
        "\n",
        "# Define the AMSA-VAE model structure\n",
        "def create_amsa_vae(input_shape, latent_dim=32):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Encoder\n",
        "    x = AMSA(d_model=128, num_heads=4)(inputs)\n",
        "    z_mean = Dense(latent_dim, activation='relu')(x)\n",
        "    z_log_var = Dense(latent_dim, activation='relu')(x)\n",
        "\n",
        "    # Sampling using reparameterization trick\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "    # Decoder\n",
        "    x_decoded = AMSA(d_model=128, num_heads=4)(z)\n",
        "    outputs = Dense(input_shape[0], activation='linear')(x_decoded)\n",
        "\n",
        "    # Define the model\n",
        "    vae = Model(inputs, outputs, name='amsa_vae')\n",
        "\n",
        "    # Define VAE loss\n",
        "    reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
        "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "    vae.add_loss(reconstruction_loss + kl_loss)\n",
        "    vae.compile(optimizer='adam')\n",
        "    return vae\n",
        "\n",
        "# Data scaling\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(df[['400017']])\n",
        "\n",
        "# Create the AMSA-VAE model\n",
        "amsa_vae = create_amsa_vae((1,))\n",
        "amsa_vae.compile(optimizer='adam', loss='mse')\n",
        "amsa_vae.fit(data_scaled, data_scaled, epochs=10, batch_size=4)\n",
        "\n",
        "# Predict and update filled data\n",
        "data_filled = amsa_vae.predict(data_scaled)\n",
        "df['400017'] = scaler.inverse_transform(data_filled).flatten()  # Update the DataFrame with repaired data"
      ],
      "metadata": {
        "id": "ariCBX2cEAvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ujFcgZJkF3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyntmuUfFG2D"
      },
      "outputs": [],
      "source": [
        "# # # Update the predicted data into dataset\n",
        "pos = 0\n",
        "for index_pos in results[0]:\n",
        "  df_1 = df.iloc[index_pos]\n",
        "  df_1['400017'] = results[1][pos]\n",
        "  # df_1['label2'] = 1\n",
        "\n",
        "  df.loc[index_pos] = df_1\n",
        "  pos+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCX6GicqFLHR"
      },
      "outputs": [],
      "source": [
        "# # # # Splitting the dataset\n",
        "# y = df['label2']\n",
        "# print(np.count_nonzero(y == 0))\n",
        "# X = df.drop(['index', 'label','label2'], axis =1)\n",
        "# print(np.count_nonzero(y == 0))\n",
        "\n",
        "# # # New noisy dataset\n",
        "df.to_csv('/content/drive/MyDrive/dataset/UNSW_missing_data/UNSW_missing_repaired_dur_01.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}