{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnLMjREJB0lu"
      },
      "outputs": [],
      "source": [
        "# # # Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLcNu1_B5xT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "74ea11a3-1144-4d08-c469-7d485ff1323a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d26e02ed546b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# # #  import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/labtest/UNSW_noisy_data/UNSW_noisy_dur_02_1_1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# # #  import data\n",
        "df = pd.read_csv('/content/drive/MyDrive/labtest/UNSW_noisy_data/UNSW_noisy_dur_05_6_5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsr-Z3C8CF1C"
      },
      "outputs": [],
      "source": [
        "# # #  Parameter configurations\n",
        "# SIGMA = 0.5\n",
        "# OUTLIER_PECENTAGE = 6\n",
        "PERCENTAGE_REPAIRED = 1724*5\n",
        "\n",
        "length = df.shape[0]\n",
        "\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "# Shuffle data\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ansqvyloDjVJ"
      },
      "outputs": [],
      "source": [
        "# # # # FID for repairing data\n",
        "# def FID_repaired(working_list):\n",
        "#     # working_list = x1\n",
        "\n",
        "#     index = []\n",
        "#     for i, j in enumerate(working_list):\n",
        "#         if j == 'NaN':\n",
        "#             index.append(i)\n",
        "\n",
        "#     # count the number of NaN/compromised point\n",
        "#     p1 = working_list.count('NaN')\n",
        "#     # print(p1)\n",
        "\n",
        "#     t = p1 # Total number of missing value\n",
        "\n",
        "#     # Select the min & max from list\n",
        "#     # working_list.remove('NaN')\n",
        "#     count=0\n",
        "#     for index_pos in index:\n",
        "#         working_list.pop(index_pos-count)\n",
        "#         count+=1\n",
        "\n",
        "#     # find mean of all observed values\n",
        "#     mean = np.mean(working_list)\n",
        "\n",
        "#     #find min value\n",
        "#     a = min(working_list)\n",
        "\n",
        "#     #find max value\n",
        "#     b = max(working_list)\n",
        "\n",
        "#     # Calculate h = (b-a)/t\n",
        "#     h = (b-a)/t\n",
        "\n",
        "#     # Calculate the discrete universe U using u = (a + (s-1) x h + a + s x h)/2, s=1,2,3\n",
        "#     U = []\n",
        "#     for s in range(1,t+1):\n",
        "#         u = (a + (s-1) * h + a + s * h)/2\n",
        "#         U.append(u)\n",
        "\n",
        "#     # print(U)\n",
        "\n",
        "#     # Calculating the missing values\n",
        "#     M = []\n",
        "#     for u in U:\n",
        "#         # print(U)\n",
        "\n",
        "#         # Compute the contribution weight (micro) of each observed element x_i\n",
        "\n",
        "#         contribution_weight_list = []\n",
        "\n",
        "#         for i in working_list:\n",
        "#             if abs(i-u) <= h:\n",
        "#                 temp = 1-(abs(i-u)/h)\n",
        "#             else:\n",
        "#                 temp = 0\n",
        "#             contribution_weight_list.append(temp)\n",
        "\n",
        "#         # Calculate the sum of x_i to u1:\n",
        "#         sum_contribution_weight_list = sum(contribution_weight_list)\n",
        "#         # print(sum_contribution_weight_list)\n",
        "\n",
        "#         # Calculate the contribution of an observed data x_i\n",
        "#         sum_contribution_observed_data = []\n",
        "\n",
        "#         for num1, num2 in zip(working_list, contribution_weight_list):\n",
        "#         \tsum_contribution_observed_data.append(num1 * num2)\n",
        "\n",
        "#         sum_contribution_observed_data = sum(sum_contribution_observed_data)\n",
        "#         # print(sum_contribution_observed_data)\n",
        "\n",
        "#         # Calculate the missing values in x_i\n",
        "#         if sum_contribution_weight_list == 0:\n",
        "#             m = mean\n",
        "#         else:\n",
        "#             m = sum_contribution_observed_data/sum_contribution_weight_list\n",
        "\n",
        "#         M.append(m)\n",
        "\n",
        "#     # print('The values:',M)\n",
        "#     # print('The index position:',index)\n",
        "#     return [index,M]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSCrjBILDxE1"
      },
      "outputs": [],
      "source": [
        "# # # # Repairing data\n",
        "# data = df['dur'].to_list()\n",
        "\n",
        "# # # # Determine the outlier for repairing\n",
        "# index_outlier = []\n",
        "\n",
        "# y = df['label2']\n",
        "# count_outlier = 0\n",
        "# for i, j in enumerate(y):\n",
        "#     if j == 0.0:\n",
        "#         index_outlier.append(i)\n",
        "#         count_outlier+=1\n",
        "#     if count_outlier == PERCENTAGE_REPAIRED: break\n",
        "\n",
        "# # Determine the compromised data\n",
        "# for ind in index_outlier:\n",
        "#     data[ind] = 'NaN'\n",
        "\n",
        "# # Recover compromised data\n",
        "# working_list = data\n",
        "\n",
        "# results = FID_repaired(working_list)\n",
        "\n",
        "# # print('The recovered values:',results[1])\n",
        "# # print('The index position:',results[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 1:\n",
        "\n",
        "# # Implementation of AMSA-VAE method for missing data filling\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# # Standard Scaling\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['dur']])\n",
        "\n",
        "# # Imputation\n",
        "# imputer = KNNImputer(n_neighbors=5)\n",
        "# data_imputed = imputer.fit_transform(data_scaled)\n",
        "\n",
        "# # AMSA-VAE Model Definition\n",
        "# input_layer = Input(shape=(1,))\n",
        "# dense_1 = Dense(128, activation='relu')(input_layer)\n",
        "# repeat_vector = RepeatVector(1)(dense_1)\n",
        "# permute_layer = Permute((2, 1))(repeat_vector)\n",
        "# attention_output = Attention()([permute_layer, permute_layer])\n",
        "# flatten_attention = Reshape((128,))(attention_output)\n",
        "# add_layer = Add()([dense_1, flatten_attention])\n",
        "# output_layer = Dense(1)(add_layer)\n",
        "\n",
        "# # Compile and Train the model\n",
        "# model = Model(inputs=input_layer, outputs=output_layer)\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "# model.fit(data_imputed, data_imputed, epochs=10, batch_size=16)\n",
        "\n",
        "# # Predict and Fill Missing Data\n",
        "# data_filled = model.predict(data_imputed)\n",
        "# data_filled = scaler.inverse_transform(data_filled)\n",
        "# df['RH'] = data_filled.flatten()  # Update the DataFrame with repaired data"
      ],
      "metadata": {
        "id": "O_JvmRF6D7hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Paper 2:\n",
        "\n",
        "# # # # Applying Switching Triple-Weight-SMOTE (NSS)\n",
        "# # Step 1: Impute missing values using LNMF\n",
        "# nmf_model = NMF(n_components=5, init='random', random_state=0)\n",
        "# W = nmf_model.fit_transform(np.nan_to_num(df[['dur']]))\n",
        "# H = nmf_model.components_\n",
        "# df['RH'] = np.dot(W, H).flatten()  # Impute missing values and update the DataFrame\n",
        "\n",
        "# # Step 2: Map to Empirical Feature Space (EFS) - simplified for demonstration\n",
        "# scaler = StandardScaler()\n",
        "# data_scaled = scaler.fit_transform(df[['dur']])\n",
        "# kmeans = KMeans(n_clusters=2, random_state=0).fit(data_scaled)\n",
        "# df['cluster'] = kmeans.labels_\n",
        "\n",
        "# # Step 3: Apply Switching Triple-Weight-SMOTE\n",
        "# # Assign synthetic samples according to cluster properties\n",
        "# for cluster_label in df['cluster'].unique():\n",
        "#     cluster_data = df[df['cluster'] == cluster_label]\n",
        "#     cluster_center = cluster_data[['dur']].mean().values\n",
        "#     distances, indices = pairwise_distances_argmin_min(cluster_data[['dur']], [cluster_center])\n",
        "#     # Synthesize new samples if needed based on cluster distribution\n",
        "#     if cluster_label == 0:  # Example condition\n",
        "#         new_samples = cluster_data.sample(frac=0.1, replace=True)\n",
        "#         df = pd.concat([df, new_samples])\n",
        "\n",
        "# # Shuffle the final DataFrame\n",
        "# df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "tJGQpPI5D-dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper 3:\n",
        "\n",
        "# AMSA-VAE implementation for data filling\n",
        "class AMSA(Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(AMSA, self).__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.norm = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = Dropout(0.1)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Expand dims to create the necessary 3D input (batch, seq_len, features)\n",
        "        x = tf.expand_dims(x, axis=1)  # Expands to (batch, 1, features)\n",
        "        attn_output = self.mha(x, x)  # Apply multi-head attention\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        out1 = self.norm(x + attn_output)  # Add & normalize\n",
        "        out1 = tf.squeeze(out1, axis=1)  # Squeeze back to 2D if necessary\n",
        "        return out1\n",
        "\n",
        "def create_amsa_vae(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = AMSA(d_model=128, num_heads=4)(inputs)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    outputs = Dense(input_shape[0], activation='linear')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Data scaling\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(df[['dur']])\n",
        "\n",
        "# AMSA-VAE model setup and training\n",
        "amsa_vae = create_amsa_vae((1,))\n",
        "amsa_vae.compile(optimizer='adam', loss='mse')\n",
        "amsa_vae.fit(data_scaled, data_scaled, epochs=10, batch_size=4)\n",
        "\n",
        "# Predict and update filled data\n",
        "data_filled = amsa_vae.predict(data_scaled)\n",
        "df['RH'] = scaler.inverse_transform(data_filled).flatten()  # Update the DataFrame with repaired data"
      ],
      "metadata": {
        "id": "ariCBX2cEAvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ujFcgZJkF3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyntmuUfFG2D"
      },
      "outputs": [],
      "source": [
        "# # # Update the predicted data into dataset\n",
        "pos = 0\n",
        "for index_pos in results[0]:\n",
        "  df_1 = df.iloc[index_pos]\n",
        "  df_1['dur'] = results[1][pos]\n",
        "  # df_1['label2'] = 1\n",
        "\n",
        "  df.loc[index_pos] = df_1\n",
        "  pos+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCX6GicqFLHR"
      },
      "outputs": [],
      "source": [
        "# # # # Splitting the dataset\n",
        "# y = df['label2']\n",
        "# print(np.count_nonzero(y == 0))\n",
        "# X = df.drop(['index', 'label','label2'], axis =1)\n",
        "# print(np.count_nonzero(y == 0))\n",
        "\n",
        "# # # New noisy dataset\n",
        "df.to_csv('/content/drive/MyDrive/dataset/UNSW_missing_data/UNSW_missing_repaired_dur_01.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}